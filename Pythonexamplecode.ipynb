{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2d1c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa45d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required libraries\n",
    "import numpy as np \n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b633549",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-14 12:51:57,215] Making new env: FrozenLake-v0\n",
      "/home/bethoveen/.local/lib/python3.10/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\") #making a ne env of frozen lake game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a535c518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n) #prints how many Actions from each statethere is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e922db20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#now we will Create our Qtable StatesxActions {0}\n",
    "#since initially all our Q values are zeros\n",
    "Q_table = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "print (Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cb7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up our parameters we discussed earlier\n",
    "\n",
    "episodes = 10000  #this refers to the number of episodes we want or agent to play\n",
    "Max_steps = 50 #this refers to the Maxnumber of steps at each episode , if exceeded the episode will end immeditely  \n",
    "learning_rate  = 0.1 # Œ± : rate of updating Q value  \n",
    "discount_rate  = 0.99 # ùõæ : rate of decay of rewards\n",
    "exploration_rate = 1 #Œµ : prob. of our agent exploring the environment instead of exploiting it\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "explortion_decay = 0.009\n",
    "rewards_all_episodes = [] # we will use this to visulatize the rewards for each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5dbcb",
   "metadata": {},
   "source": [
    "***quick reminder of what we want to do in the algorithm***\n",
    "\n",
    "1) at the beggining of each episdoe we need to reset the state to inital\n",
    "2) now for each step in the episdoe we need to randomly select wether to explore or exploit according to a random value r realtive to the Œµ value (r<Œµ -> explore) (r>Œµ -> exploit)\n",
    "3) take an acction with explor-exploit in-mind\n",
    "4) updating our Q-table values\n",
    "5) then set our state to the new state and add the reward to the total rewards of the episode\n",
    "6) finally we would update Œµ using exploration decay rate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a52fe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# ****Q-Learning algorithm****\n",
    "for episode in range(episodes):\n",
    "    current_state = env.reset() #at the beggining of each episode we set state to initial state\n",
    "    ep_done = False\n",
    "    total_episode_reward = 0\n",
    "\n",
    "    #steps code\n",
    "    for step in range(Max_steps):\n",
    "        #uncomment to see Agent playing\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        #exploration vs epoitation\n",
    "        r = random.uniform(0,1)\n",
    "        if(r<exploration_rate):\n",
    "            #explore action\n",
    "            action  = env.action_space.sample()\n",
    "        else:\n",
    "            #exploit action\n",
    "            action = np.argmax(Q_table[current_state,:])\n",
    "        \n",
    "        #taking the action    \n",
    "        new_state, reward, ep_done,info  = env.step(action)\n",
    "        \n",
    "        #updating Q_table values\n",
    "        Q_table[current_state,action] = (1-learning_rate) * Q_table[current_state,action] + learning_rate * (reward + discount_rate * (np.max(Q_table[new_state,:])))\n",
    "        current_state = new_state\n",
    "        total_episode_reward += reward\n",
    "        \n",
    "        #checking if the step made us end the game\n",
    "        if ep_done == True:\n",
    "            #print(reward)\n",
    "            break; #then end the episode without taking any other steps\n",
    "    \n",
    "    #now updating the exploration rate\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-explortion_decay * episode)\n",
    "    rewards_all_episodes.append(total_episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10366481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.0\n",
      "2000 :  0.0\n",
      "3000 :  0.0\n",
      "4000 :  0.0\n",
      "5000 :  0.0\n",
      "6000 :  0.0\n",
      "7000 :  0.0\n",
      "8000 :  0.0\n",
      "9000 :  0.0\n",
      "10000 :  0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a9284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbd517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8dc94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
